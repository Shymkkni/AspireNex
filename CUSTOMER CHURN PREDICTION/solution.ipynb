{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task is pretty easy to understand. But, to brief it, we have 21 different columns where CHURN is Y(dependent variable) and customerID is not needed and remaining 19 are X(dependent variables). Now we try all 3 classification methods i.e. Random forest, logistic regression and Gradient boosting. All the models has almost same accuracy, but Gradient Boosting has more accuracy. After finnetuning Randomforest using GridSearchCV, the accuracy of the model increases to 80.24%. Since for finetuning, we have to choose parameters manually(Hyperparameters), I have tried many possible combinations of parameters. The parameters with most accuracy of 80 percent are as follows:\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2]\n",
    "}\n",
    "After finetuning the model, we can see that random forest's accuracy increases from 79.5 to 80.2 percent. That's why we choose random forest as final model for training and prediction.\n",
    "I have loaded the model in a .pkl file using joblib.\n",
    "If the user wants to predict output using this model, simpliy run the model using joblib.load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv('CHURN_DATASET.csv')\n",
    "\n",
    "df = df[df['TotalCharges'] != ' ']\n",
    "df['TotalCharges'] = df['TotalCharges'].astype(float)\n",
    "\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    if column != 'customerID':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "X = df.drop(['customerID', 'Churn'], axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1033\n",
      "           1       0.62      0.49      0.55       374\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.73      0.69      0.70      1407\n",
      "weighted avg       0.77      0.79      0.78      1407\n",
      "\n",
      "Accuracy: 0.7853589196872779\n",
      "ROC AUC: 0.6926311402850324\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86      1033\n",
      "           1       0.64      0.48      0.55       374\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.73      0.69      0.71      1407\n",
      "weighted avg       0.78      0.79      0.78      1407\n",
      "\n",
      "Accuracy: 0.7903340440653873\n",
      "ROC AUC: 0.6917549735726376\n",
      "\n",
      "Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87      1033\n",
      "           1       0.65      0.49      0.56       374\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.74      0.70      0.71      1407\n",
      "weighted avg       0.78      0.80      0.79      1407\n",
      "\n",
      "Accuracy: 0.7953091684434968\n",
      "ROC AUC: 0.6985546484720792\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_logreg))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_rf))\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"\\nGradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy is :  0.7853589196872779\n",
      "Random forest accuracy after finetuning is:  0.8024164889836531\n",
      "Gradient boosting accuracy is:  0.7953091684434968\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "logreg_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "logreg_roc_auc = roc_auc_score(y_test, y_pred_logreg)\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_roc_auc = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "gb_roc_auc = roc_auc_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"Logistic Regression accuracy is : \",logreg_accuracy)\n",
    "print(\"Random forest accuracy after finetuning is: \",rf_accuracy)\n",
    "print(\"Gradient boosting accuracy is: \",gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForest.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_rf, 'RandomForest.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
